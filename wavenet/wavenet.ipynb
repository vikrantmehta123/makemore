{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have tried to implement the Makemore: Part 5 from Andrej Karpathy's series. This notebook is my implementation, along with the notes that I took while following along the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataset\n",
    "\n",
    "Same as always- we're creating training, test, and validation datasets where each training input is a three character sequence and the output is the 4th character that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "words = open(r'../names.txt', 'r').read().splitlines()\n",
    "len(words), words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary and Basic Character encoding and decoding\n",
    "\n",
    "chars = sorted(list(set(''.join(words)))) # Get all the unique chars in sorted order\n",
    "\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(\"itos: \", itos)\n",
    "print(\"Vocab Size is: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 \n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [ ], [ ]\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size # For start of the word, have a padded context\n",
    "        for char in w + \".\":\n",
    "            idx = stoi[char]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validation, and test split\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_train, Y_train = build_dataset(words=words[:n1]) # 80% of data\n",
    "X_val, Y_val = build_dataset(words=words[n1:n2]) # 10% of data\n",
    "X_test, Y_test = build_dataset(words=words[n2:]) # 10% of data\n",
    "\n",
    "print(\"Train sizes: \", X_train.shape, Y_train.shape)\n",
    "print(\"Validation Sizes: \", X_val.shape, Y_val.shape)\n",
    "print(\"Test Sizes: \", X_test.shape, Y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
