{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GPT from Scratch\n",
    "\n",
    "In this notebook, I implement a GPT from scratch following Andrej Karpathy's YouTube series, along with my notes of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "lr = 1e-3\n",
    "epochs = 10_000\n",
    "eval_interval = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset\n",
    "\n",
    "We create the vocabulary and the dataset as we have been doing in the other `makemore` notebooks. But here, instead of using the `names` dataset, we will be using Tiny Shakespeare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of the dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are building a character level language model. Our vocabulary is going to be all the characters in the dataset, and the *tokens* in our language model are the characters mapped to integers. In LLMs, this tokenization could be at *subword* level, or something else also! \n",
    "\n",
    "The larger the vocabulary, the larger integer to token mapping you have. That means, that you can represent larger sentences using fewer tokens. On the contrary, if you have less number of tokens in your vocabulary, you will need more tokens to represent larger sentence. \n",
    "\n",
    "For example, with character level language model, we need `len(sentence)` tokens to represent it. But if we had a word level tokenization, then we would need `len(sentence.split(\" \"))` tokens, which would be fewer than the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size is:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab Size is: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 47, 6, 1, 58, 46, 43, 56, 43, 2]\n",
      "Hii, there!\n"
     ]
    }
   ],
   "source": [
    "# Create an integer to character mapping- i.e. the tokenizer that encodes and decodes tokens\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # takes an input string, and outputs a list of integers. i.e. the character map\n",
    "decode = lambda l: \"\".join([itos[i] for i in l]) # takes the token list, and produces the string for it\n",
    "\n",
    "print(encode(\"Hii, there!\"))\n",
    "print(decode(encode(\"Hii, there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the text into PyTorch tensor now, and split the encoded dataset into train and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(0.9 * len(data))\n",
    "train_data = data[:cut]\n",
    "validation_data = data[cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the context length first. This context length is the maximum context that the model can look at when making a prediction. However, there doesn't have to be 8 characters always- you can have less than that. Thus, you get something as this. But notice that now we're dealing with tokens and not integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: maximum 8 tokens can be taken as context\n",
    "\n",
    "sample_x = train_data[:block_size]\n",
    "sample_y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = sample_x[:t+1]\n",
    "    target = sample_y[t]\n",
    "\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Batches\n",
    "\n",
    "Now that the dataset is there, we need to think about how the input text can be passed as a batch. \n",
    "\n",
    "Before that, an important thing to note about transformers is that there is a maximum number of tokens that you can pass to them. They are able to handle sequential inputs of arbitrary length, but this arbitrary length is also capped to some number such as 512. This number is the context length. You can have at maximum that many tokens but at minimum, you can have any number of tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about how would we create and pass a batch of sequences to the model. Our wishlist is the following:\n",
    "\n",
    "1. We want to pick arbitrary sequences so that the model can generalize well. How do we pick random sequences? Just pick out random starting indexes.\n",
    "2. How big a sequence should you pick? Well, it cannot be more than the context length of the model. For the moment, assume you would pick the input of size `block_size` i.e. the context size. For example, if you have a `batch_size` of 4 and `block_size` of 8, then you would randomly pick 4 indices in the dataset, and index 8 characters from that index. \n",
    "3. What should be the targets? The targets are just the next character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, one sequence of 8 characters gives us 8 training examples ( in cell above ). So when we have a batch of size 4, with each having a sequence of 32, it is going to give 32 training samples. \n",
    "\n",
    "**Important:** Each training sample can be passed independently to the transformer!\n",
    "\n",
    "The key is going to be figuring out how to pass this to the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs: torch.Size([4, 8])\n",
      "Shape of outputs: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split:str):\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # randomly select batch_size many indices. len(data) - block_size just handles edge case\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f\"Shape of inputs: {xb.shape}\")\n",
    "print(f\"Shape of outputs: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this batched input, we can again split the training examples. But note, this is NOT relevant till we get to transformers. At the moment, we are just training a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: When input is tensor([24]) the target is: 43\n",
      "1: When input is tensor([24, 43]) the target is: 58\n",
      "2: When input is tensor([24, 43, 58]) the target is: 5\n",
      "3: When input is tensor([24, 43, 58,  5]) the target is: 57\n",
      "4: When input is tensor([24, 43, 58,  5, 57]) the target is: 1\n",
      "5: When input is tensor([24, 43, 58,  5, 57,  1]) the target is: 46\n",
      "6: When input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is: 43\n",
      "7: When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is: 39\n",
      "8: When input is tensor([44]) the target is: 53\n",
      "9: When input is tensor([44, 53]) the target is: 56\n",
      "10: When input is tensor([44, 53, 56]) the target is: 1\n",
      "11: When input is tensor([44, 53, 56,  1]) the target is: 58\n",
      "12: When input is tensor([44, 53, 56,  1, 58]) the target is: 46\n",
      "13: When input is tensor([44, 53, 56,  1, 58, 46]) the target is: 39\n",
      "14: When input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is: 58\n",
      "15: When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is: 1\n",
      "16: When input is tensor([52]) the target is: 58\n",
      "17: When input is tensor([52, 58]) the target is: 1\n",
      "18: When input is tensor([52, 58,  1]) the target is: 58\n",
      "19: When input is tensor([52, 58,  1, 58]) the target is: 46\n",
      "20: When input is tensor([52, 58,  1, 58, 46]) the target is: 39\n",
      "21: When input is tensor([52, 58,  1, 58, 46, 39]) the target is: 58\n",
      "22: When input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is: 1\n",
      "23: When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is: 46\n",
      "24: When input is tensor([25]) the target is: 17\n",
      "25: When input is tensor([25, 17]) the target is: 27\n",
      "26: When input is tensor([25, 17, 27]) the target is: 10\n",
      "27: When input is tensor([25, 17, 27, 10]) the target is: 0\n",
      "28: When input is tensor([25, 17, 27, 10,  0]) the target is: 21\n",
      "29: When input is tensor([25, 17, 27, 10,  0, 21]) the target is: 1\n",
      "30: When input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is: 54\n",
      "31: When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is: 39\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension ( PyTorch convention: (B, T, C) = (Batch, Time, Channel))\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"{i}: When input is {context} the target is: {target}\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Self-Attention Block\n",
    "\n",
    "The bigram model was not paying any attention to the context. It was just considering the previous character. Now, we will build a self-attention block that pays attention to context of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention, Mathematically\n",
    "\n",
    "Think about what attention means. Attention is just a mechanism to let the past context interact with the current token. How do you make two vectors *interact*? By the way of addition and multiplication.\n",
    "\n",
    "One simple way of encoding context would be to take average of embeddings of all the previous tokens in the context, and remember that the context length is fixed. But what taking a simple average means is that the current token is paying equal attention to the all the previous tokens in the context. We don't want that. We want the attention to be data dependent, which in other words means that this attention should be *learned* from the model.\n",
    "\n",
    "So we're going to take a weighted average of the previous tokens but these weights are going to be learned by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights need to sum up to 1. And each token should only look at itself and the previous token- not the tokens that come after it.\n",
    "\n",
    "How do you encode this operation mathematically? Why, matrix multiplication, of course! If you think about it, if you have a lower diagonal weight matrix, then you can multiply it by the context and you're going to satisfy the condition that each token should only look at the token that comes before it.\n",
    "\n",
    "```\n",
    "weights @ context\n",
    "```\n",
    "\n",
    "In PyTorch, `torch.tril` gives us a lower triangular matrix. You need to pass the matrix that you want to convert to lower triangular.\n",
    "\n",
    "How do we encode a simple average with this matrix multiplication?\n",
    "Just divide each row by the number of non-zero entries in it. Think about it, in a lower triangular matrix, everything above the main diagonal is zero. So if you divide the each row by the number of non-zero entries in it, then essentially you are taking average. See the example below for the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do matrix multiplication to get the simple average. But think about the dimensions of this multiplication.\n",
    "\n",
    "```\n",
    "weights.shape = (T, T)\n",
    "x.shape = (B, T, C)\n",
    "```\n",
    "\n",
    "So, PyTorch is going to *slide* the matrix multiplication across the batch dimension and we're going to get an output that is of shape: ```(B, T, C)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = weights @ x\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main requirement is that these attention weights need to be trainable. For the moment, they are not. So we need a way that allows us to learn those attention weights while still preserving the averaging mechanism.\n",
    "\n",
    "That's where softmax is going to come in. Look up Mitesh Sir's Teacher Forcing or Masked Attention lectures for the intuition behind the `-inf` values. But essentially, these `-inf` and `0` values achieve the same thing as before:\n",
    "\n",
    "1. They let the model only focus on previous tokens\n",
    "2. Each row sums to 1.\n",
    "\n",
    "Because:\n",
    "1. $softmax(- \\infty) = 0$\n",
    "2. $exp(0) = 1$\n",
    "\n",
    "What we want is that the weights at each row should sum to 1 and they should be learnable. We know a function that lets generate a row that sums to 1 and that is softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # wherever tril == 0, replace with -inf\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can do matrix multiplication and we get the same matrix as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights @ x).allclose(xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a moment to appreciate what is happening here. For each token, we said we would combine the information from the previous tokens by addition and taking the average. With this mask, we effectively achieve the same. We prevent the information present in the future tokens to be combined with the present token because that information is going to get multiplied by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `weights` matrix is essentially what is going to be defining the how much attention to pay to each of the previous tokens given a certain token. We want the model to learn this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Self-Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's think about how self-attention is implemented. Each token (of the block_size tokens) *emits* two vectors- a key and a query. You'll introduce a Key and Query matrix as parameters which transform a given token. Then linear transformation of token with Key matrix gives the key vector and similarly for the query vector.\n",
    "\n",
    "- **Key Vector:** Captures \"What I contain\"\n",
    "- **Query Vector:** Captures \"What am I looking for\"\n",
    "\n",
    "So far, there is no communication between tokens.\n",
    "\n",
    "But then, the way we're going to find whether a token is worth paying attention to is by doing dot product of queries and keys. This is how we're going to find attention weights.   For each token, you want to find out whether the other tokens contain what the current token is looking for. If the dot product of these two vectors is high, then that means that the key vector contains what the query vector was looking for. \n",
    "\n",
    "This dot product is what the attention weights matrix will be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16 # hyperaparam\n",
    "key = nn.Linear(C, head_size, bias=False) # W_K matrix\n",
    "query = nn.Linear(C, head_size, bias=False) # W_Q matrix\n",
    "\n",
    "# Apply linear transforms to get key and query vectors for each\n",
    "k = key(x) # shape is (B, T, 16)\n",
    "q = query(x) # shape is (B, T, 16)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) # Transpose T, C of `k`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small comment on the matrix multiplication of `q` and `k`. This is essentially going to achieve pairwise dot product of queries and keys. But notice that we have a batch dimension. We don't want to use batch dimension when doing matrix multiplication. So we need to transpose only the last two dimensions. So the shape of weights now is: `(B, T, T)`. $ T \\times T$ is what we want- pairwise attention weights, and we have those pairwise attention weights for each the batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `weights`, we can do what we did before, we can mask it and apply softmax to get attention weighted aggregates of the previous tokens. Again, these are just weights to take a weighted average of the previous tokens such that the information in them is encoded in the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5499, 0.4501, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2910, 0.2318, 0.4771, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3559, 0.2289, 0.2161, 0.1991, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2080, 0.2243, 0.1799, 0.2138, 0.1741, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1528, 0.1884, 0.1435, 0.1584, 0.1480, 0.2089, 0.0000, 0.0000],\n",
       "        [0.1435, 0.1233, 0.1774, 0.1236, 0.1706, 0.1296, 0.1320, 0.0000],\n",
       "        [0.1108, 0.1917, 0.1099, 0.1093, 0.1100, 0.1352, 0.1153, 0.1177]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "weights = weights.masked_fill(tril==0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ x\n",
    "\n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `weights` now contains the attention weights to take the weighted average of only the past tokens and they are not uniform- so we have learnable parameters now!\n",
    "\n",
    "And `out` is now the modified input `x` by taking the weighted average using the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one additional component in the self attention block. That is, we don't modify the input `x` directly. We pass `x` through a linear transformation to produce a `value` for that input and then we modify this value. \n",
    "\n",
    "Another way of thinking about this is that the information contained in the `x` is private. So it tells the other vectors what it contains (key), what it is looking for (query), and what it will communicate with them (value). So we need a linear transform, which again are parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size)\n",
    "v = value(x)\n",
    "out = weights @ v\n",
    "out.shape # out shape is now 16D instead of 32D as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this Q, K, V matrix multiplications and the weighted aggregation constitutes one *head* of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andrej's Notes on Self-Attention\n",
    "\n",
    "1. Attention is just a communication mechanism- not specific to language modeling. You can look at it as a directed graph where each node points to other nodes with which it is communicating, and the weights of those edges? The attention weights! So you can apply attention to ANY directed graph!\n",
    "\n",
    "2. There is no notion of space in attention. Attention simply acts over a set of vectors. So if you have a problem where the sequence is important, then you need to encode the positional information, like we did.\n",
    "\n",
    "3. There is no communication across batches. With the matrix multiplications, the elements within a batch communicate but across batches there is no communication. \n",
    "\n",
    "4. Not all tasks require you to have a masked attention. In this task, we wanted to predict the future token and thus we didn't want the tokens to communicate with the future. But let's say your task is sentiment analysis, then there is no need for masking. You will have the entire sentiment with you and you can allow the communication with future tokens also. Without masking the self-attention block is called an \"encoder block\", and with masking it is called the \"decoder block\". Look up Mitesh sir's lectures on Encoder only and Decoder only models. Attention by itself has no notion about which nodes it can communicate with.\n",
    "\n",
    "5. Self-attention means that the keys, queries, and values are coming from same source- the input `x`. But there is cross-attention also where the queries come from different source and the keys & values come from an entirely different source, such as an encoder-decoder architecture.\n",
    "\n",
    "6. In the original \"Attention is All You Need\" paper, we have the equation:\n",
    "$softmax(\\dfrac{QK^T}{\\sqrt{d_k}})V$. We have done all the matrix multiplication part but there is also `sqrt(head_size)`. The attention is being *scaled* so it's called Scaled attention, and it is to make sure that when the input to Q and K is unit variance, then the `weights` is unit variance too, so as to avoid saturation when being passed through the softmax. So at least at initialization, you want `weights` to avoid taking too extreme values. So when we are doing the `q @ k` matrix multiplication, we need to multiply it by `C ** -0.5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Head Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32 # this is a global variable\n",
    "block_size = 8\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "\n",
    "        # TODO: need to understand indexing in tril for this line. It's most likely to handle less than block_size inputs\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf') )\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "\n",
    "        out = weights @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, whatever is not a parameter of the model is called a `buffer`. It is just creating the `tril` variable and assigning it to the module but it is not a parameter of the module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Language Model With Attention Head\n",
    "\n",
    "We introduce an additional linear layer called `lm_head` which will transform the output of attention head to match the dimensions of the targets. \n",
    "\n",
    "We also need to make sure that there are at maximum only `block_size` tokens that are passed to the model, otherwise the `position_embedding_table`, which captures the positional information upto block size, is going to throw an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro Tip:** Self-attention cannot handle very high learning rates. So you need something like `1e-3` or lower, and you need to train for more number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "block_size = 8\n",
    "\n",
    "class LangModelWithOneHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "\n",
    "        # Final linear layer to produce logits of equal dim as targets\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Combine the information of the token and the position\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # pass the position + token information through the attention head\n",
    "        x = self.sa_head(x) # -> (B, T, head_size)\n",
    "\n",
    "        # produce logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # You can't have more than block_size tokens in context now because pos_embd will throw an error.\n",
    "            # so keep only the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, loss = self(idx_cond) # logits.shape is (4, x, 65)\n",
    "\n",
    "            # same as bigram\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        \n",
    "        # idx will be the sequence generated for each batch\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Attention Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LangModelWithOneHead()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want a smoother loss estimate since the loss can vary batch to batch based on what sample is drawn. So we do what we did before to smooth the loss estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = { }\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # for train and val data, take mean of 300 iters\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # set model back to train mode\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.2181, and val loss:4.2123\n",
      "Step 1000: train loss 2.8455, and val loss:2.8353\n",
      "Step 2000: train loss 2.5760, and val loss:2.6110\n",
      "Step 3000: train loss 2.5465, and val loss:2.4903\n",
      "Step 4000: train loss 2.4745, and val loss:2.5097\n",
      "Step 5000: train loss 2.4687, and val loss:2.5062\n",
      "Step 6000: train loss 2.4688, and val loss:2.4536\n",
      "Step 7000: train loss 2.4384, and val loss:2.4494\n",
      "Step 8000: train loss 2.4328, and val loss:2.4681\n",
      "Step 9000: train loss 2.4604, and val loss:2.4686\n",
      "Loss is: 2.6558\n",
      "\n",
      "Generated sequence: \n",
      "\n",
      "Sthithang angh drar did yo my pulas grires lounowm tor.\n",
      "\n",
      "Pr ing\n",
      "I acrevont, atheen thitanimis BEfe no.\n",
      " gep,\n",
      "Wif sere\n",
      "no age Your wlo, phourind ind wi\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {epoch}: train loss {losses['train']:.4f}, and val loss:{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch from the dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\\n\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=150)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is slightly better but it needs to be a whole lot better than this also, as the text being produced is still not as impressive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having just one attention head, we can have multiple attention heads and we can concatenate their outputs, and then we can pass this concatenated output to the feedforward neural network. This is much like convolutions. You train multiple small filters in the hopes that they specialize in capturing something very specific. Similarly, here also, you have multiple smaller heads instead of one big one in similar hopes. Further, we want all these attention heads to be running in parallel. \n",
    "\n",
    "Running parallely is very easy to do in PyTorch and since we're only concatenating the outputs, the heads are also easy to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multiple attention heads running in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # all heads run in parallel\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can include this multi-headed attention in the language model. But there is one more component that is there in the \"Attention is All You Need\" paper, that we need to add in the model. And that is the feedforward network. \n",
    "\n",
    "Currently, we have one linear layer at the end to produce logits. The feedforward neural network that we want is a simple MLP, but why is this needed?\n",
    "\n",
    "Think of it this way: The multi-headed attention did the communication part. It found out and reflected which context tokens are important for each token. Then if we just have a linear layer that directly produces the logits, then we are going too fast! We need to let the model *think* on what it found on the other tokens. So to allow the model to do this, we add a linear layer also. We add a simple MLP here.\n",
    "\n",
    "All the tokens have gathered information about their specific attentions via the attention heads. Now, they need to *think* on this information on their own. So this linear level is applied on each of the individual token independently, without worrying about the context now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Simple MLP followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModelWithMultiHead(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadedAttention(4, n_embd // 4) # 4 heads of size (32 / 4 = 8)\n",
    "        \n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        # Final linear layer to produce logits of equal dim as targets\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Combine the information of the token and the position\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        # pass the position + token information through the attention heads\n",
    "        x = self.sa_heads(x) # For this example, the output is: (B, T, 32)\n",
    "\n",
    "        # Call feedforward\n",
    "        x = self.ffwd(x) # -> (B, T, n_embd=32)\n",
    "\n",
    "        # produce logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # keep only the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, loss = self(idx_cond) # logits.shape is (4, x, 65)\n",
    "\n",
    "            # same as bigram\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        \n",
    "        # idx will be the sequence generated for each batch\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a language model that has one attention block. In the transformer architecture, you have multiple such blocks that are stacked one after the other. So we are interspersing the attention, followed by linear, followed again by attention and so on.\n",
    "\n",
    "But for the moment let's train this single block model and see the output that we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LangModelWithMultiHead()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.2029, and val loss:4.2021\n",
      "Step 1000: train loss 2.7353, and val loss:2.7016\n",
      "Step 2000: train loss 2.5472, and val loss:2.5305\n",
      "Step 3000: train loss 2.4571, and val loss:2.4705\n",
      "Step 4000: train loss 2.4199, and val loss:2.4289\n",
      "Step 5000: train loss 2.4011, and val loss:2.4005\n",
      "Step 6000: train loss 2.3493, and val loss:2.3913\n",
      "Step 7000: train loss 2.3530, and val loss:2.3418\n",
      "Step 8000: train loss 2.3074, and val loss:2.3463\n",
      "Step 9000: train loss 2.2718, and val loss:2.3193\n",
      "Loss is: 2.3365\n",
      "\n",
      "Generated sequence: \n",
      "\n",
      "KNIENGLORWERNGERNLESS:'s\n",
      "And led frour she, a thave\n",
      "Whe canle, seld spnow froscoune maghe to cout gom.\n",
      "Thed, the Maim mart dent\n",
      "Halor; cot hert rut ho\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {epoch}: train loss {losses['train']:.4f}, and val loss:{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch from the dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\\n\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=150)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is significantly improved. But the text still isn't great, however we are able to see some words there. We need to improve this loss even more, which we will do by stacking up these multi-headed attention blocks. But simply stacking up these blocks won't necessarily improve performance. Why?\n",
    "\n",
    "Because stacking up the blocks will make the neural network quite deep, and you need things like batch & layer normalization, skip connections, etc. to be able to successfully train deep networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block With Skip Connections\n",
    "\n",
    "We will implement the block with skip connections, as was introduced in the paper. But let's first think a bit about skip connections. Refer to FastAI course notebook on ResNets to know the basics of skip connections. But here, I am adding Andrej's take on Skip Connections which is super intuitive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections\n",
    "\n",
    "Skip connections is one way of letting us train deeper neural networks. \n",
    "\n",
    "![Skip Connection](skip-connection.png)\n",
    "\n",
    "You have a residual pathway that directly produces the output. But you also have an option to drift off and go into the other direction, do some computation, and add this output to the residual output.\n",
    "\n",
    "One important thing about this is that the residual connection and the output is combined via addition. Addition operation equally distributes gradients to all the branches, in this case- the residual path and the other branch. \n",
    "\n",
    "So, by allowing this residual connection, the gradients have a highway to flow, instead of getting saturated because of the depth.\n",
    "\n",
    "Further, the \"branches\" are initialized such that they contribute very little, if at all, at the beginning. So initially, we're pretty much training a shallow model. But over time, we hope that these branches will learn something that is not known to the shallow model and improve the model performance. \n",
    "\n",
    "This lets us train deeper network as if the deeper network has nothing to contribute, the model can learn to *ignore* the deep layers as it has a shallow network within it also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "It's very similar to batch normalization, and we're going to implement it directly inside the code block. In the original paper, LayerNorm was applied after the feedforward and the attention blocks. But now, it is much more common to apply layer normalization before these transformations. It's called \"pre-norm\" formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement residual connections and refactor the attention mechanism into transformer blocks. We will also add dropouts to avoid overfitting. Dropouts are generally added as the last layer before the residual and the branch is merged into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "\n",
    "        # TODO: need to understand indexing in tril for this line. It's most likely to handle less than block_size inputs\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf') )\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        v = self.value(x)\n",
    "\n",
    "        out = weights @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Single Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embd // n_head # make head size smaller based on num of heads\n",
    "        self.sa = MultiHeadedAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # branch off, do some computation (LayerNorm applied before transformation) and come back i.e. skip connection\n",
    "        x = x + self.ffwd(self.ln2(x)) # branch off, do some computation (LayerNorm applied before transformation) and come back i.e. skip connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one slight problem here. That is in the `forward` method of this block. We have naively added the current input with the output of the self-attention block and the feedforward block. But this addition may not be compatible as the dimensions may change. So in the feedforward and the self-attention layers, we need to introduce a projection that maps their output to the same dimension as the input and so that we can perform the addition mentioned in the `forward` method of the `Block`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the dimension of inner layer of feedforward network is four times the dimensionality of the self-attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multiple attention heads running in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # all heads run in parallel\n",
    "        self.proj = nn.Linear( n_embd, n_embd) # (the output of self-attention, the required output). In this case, both are same. But they need not be\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Simple MLP followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear( 4 * n_embd, n_embd), # (dim of output of ffwd, the required dim). In this case, both are same. But they need not be\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the architecture for this, train the model and see the output now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, n_blocks, block_size, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential( *[Block(n_embd, n_head) for _ in range(n_blocks)]  )\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Final linear layer to produce logits of equal dim as targets\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Combine the information of the token and the position\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        # pass the position + token information through the attention heads\n",
    "        x = self.blocks(x) # For this example, the output is: (B, T, 32)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # produce logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # keep only the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, loss = self(idx_cond) # logits.shape is (4, x, 65)\n",
    "\n",
    "            # same as bigram\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        \n",
    "        # idx will be the sequence generated for each batch\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModelTransformer(n_blocks=4, n_head=4, block_size=8, n_embd=32)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.3513, and val loss:4.3609\n",
      "Step 1000: train loss 2.5287, and val loss:2.5301\n",
      "Step 2000: train loss 2.4189, and val loss:2.3963\n",
      "Step 3000: train loss 2.3714, and val loss:2.4043\n",
      "Step 4000: train loss 2.3527, and val loss:2.3573\n",
      "Step 5000: train loss 2.3200, and val loss:2.3082\n",
      "Step 6000: train loss 2.2617, and val loss:2.2762\n",
      "Step 7000: train loss 2.2245, and val loss:2.2669\n",
      "Step 8000: train loss 2.2005, and val loss:2.2368\n",
      "Step 9000: train loss 2.1924, and val loss:2.2188\n",
      "Loss is: 2.2543\n",
      "\n",
      "Generated sequence: \n",
      "\n",
      "\n",
      "I:\n",
      "But thour geing?\n",
      "Thats Ieed bet hey,\n",
      "A:\n",
      "And nome singnbay:\n",
      "KI with sad, balll hingt or a stam's tio man and thif at y,maper lemens and with shor's than:\n",
      "\n",
      "onish nochas tiull lomede porsit uyrron.\n",
      "Comydfolgeae muall takepto as ham tnou dit tho\n",
      "to c\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {epoch}: train loss {losses['train']:.4f}, and val loss:{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch from the dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\\n\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=250)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the building blocks to train a much deeper model. So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "epochs = 5000\n",
    "eval_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_blocks = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModelTransformer(n_blocks=n_blocks, n_head=n_head, block_size=block_size, n_embd=n_embd)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {epoch}: train loss {losses['train']:.4f}, and val loss:{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch from the dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\\n\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=250)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained this model on Google Colab with GPU. After 20,000 epochs, I had to reduce the learning rate by dividing it by 10, as the loss was not decreasing and was oscillating between 1.90 to 1.95. After decreasing it, I got a better loss at around 1.85. After running at this learning rate of `3e-5` for 10,000 epochs I got a better loss of around 1.80. After repeating this process several times, we get a comparable performance as Andrej's lecture. So here's my tiny Shaekespeare when it outputs 10,000 characters:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
