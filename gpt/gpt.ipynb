{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GPT from Scratch\n",
    "\n",
    "In this notebook, I implement a GPT from scratch following Andrej Karpathy's YouTube series, along with my notes of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset\n",
    "\n",
    "We create the vocabulary and the dataset as we have been doing in the other `makemore` notebooks. But here, instead of using the `names` dataset, we will be using Tiny Shakespeare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of the dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are building a character level language model. Our vocabulary is going to be all the characters in the dataset, and the *tokens* in our language model are the characters mapped to integers. In LLMs, this tokenization could be at *subword* level, or something else also! \n",
    "\n",
    "The larger the vocabulary, the larger integer to token mapping you have. That means, that you can represent larger sentences using fewer tokens. On the contrary, if you have less number of tokens in your vocabulary, you will need more tokens to represent larger sentence. \n",
    "\n",
    "For example, with character level language model, we need `len(sentence)` tokens to represent it. But if we had a word level tokenization, then we would need `len(sentence.split(\" \"))` tokens, which would be fewer than the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size is:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab Size is: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 47, 6, 1, 58, 46, 43, 56, 43, 2]\n",
      "Hii, there!\n"
     ]
    }
   ],
   "source": [
    "# Create an integer to character mapping- i.e. the tokenizer that encodes and decodes tokens\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # takes an input string, and outputs a list of integers. i.e. the character map\n",
    "decode = lambda l: \"\".join([itos[i] for i in l]) # takes the token list, and produces the string for it\n",
    "\n",
    "print(encode(\"Hii, there!\"))\n",
    "print(decode(encode(\"Hii, there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the text into PyTorch tensor now, and split the encoded dataset into train and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(0.9 * len(data))\n",
    "train_data = data[:cut]\n",
    "validation_data = data[cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the context length first. This context length is the maximum context that the model can look at when making a prediction. However, there doesn't have to be 8 characters always- you can have less than that. Thus, you get something as this. But notice that now we're dealing with tokens and not integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: maximum 8 tokens can be taken as context\n",
    "\n",
    "sample_x = train_data[:block_size]\n",
    "sample_y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = sample_x[:t+1]\n",
    "    target = sample_y[t]\n",
    "\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Batches\n",
    "\n",
    "Now that the dataset is there, we need to think about how the input text can be passed as a batch. \n",
    "\n",
    "Before that, an important thing to note about transformers is that there is a maximum number of tokens that you can pass to them. They are able to handle sequential inputs of arbitrary length, but this arbitrary length is also capped to some number such as 512. This number is the context length. You can have at maximum that many tokens but at minimum, you can have any number of tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about how would we create and pass a batch of sequences to the model. Our wishlist is the following:\n",
    "\n",
    "1. We want to pick arbitrary sequences so that the model can generalize well. How do we pick random sequences? Just pick out random starting indexes.\n",
    "2. How big a sequence should you pick? Well, it cannot be more than the context length of the model. For the moment, assume you would pick the input of size `block_size` i.e. the context size. For example, if you have a `batch_size` of 4 and `block_size` of 8, then you would randomly pick 4 indices in the dataset, and index 8 characters from that index. \n",
    "3. What should be the targets? The targets are just the next character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, one sequence of 8 characters gives us 8 training examples ( in cell above ). So when we have a batch of size 4, with each having a sequence of 32, it is going to give 32 training samples. \n",
    "\n",
    "**Important:** Each training sample can be passed independently to the transformer!\n",
    "\n",
    "The key is going to be figuring out how to pass this to the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs: torch.Size([4, 8])\n",
      "Shape of outputs: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split:str):\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # randomly select batch_size many indices. len(data) - block_size just handles edge case\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f\"Shape of inputs: {xb.shape}\")\n",
    "print(f\"Shape of outputs: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this batched input, we can again split the training examples. But note, this is NOT relevant till we get to transformers. At the moment, we are just training a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: When input is tensor([24]) the target is: 43\n",
      "1: When input is tensor([24, 43]) the target is: 58\n",
      "2: When input is tensor([24, 43, 58]) the target is: 5\n",
      "3: When input is tensor([24, 43, 58,  5]) the target is: 57\n",
      "4: When input is tensor([24, 43, 58,  5, 57]) the target is: 1\n",
      "5: When input is tensor([24, 43, 58,  5, 57,  1]) the target is: 46\n",
      "6: When input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is: 43\n",
      "7: When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is: 39\n",
      "8: When input is tensor([44]) the target is: 53\n",
      "9: When input is tensor([44, 53]) the target is: 56\n",
      "10: When input is tensor([44, 53, 56]) the target is: 1\n",
      "11: When input is tensor([44, 53, 56,  1]) the target is: 58\n",
      "12: When input is tensor([44, 53, 56,  1, 58]) the target is: 46\n",
      "13: When input is tensor([44, 53, 56,  1, 58, 46]) the target is: 39\n",
      "14: When input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is: 58\n",
      "15: When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is: 1\n",
      "16: When input is tensor([52]) the target is: 58\n",
      "17: When input is tensor([52, 58]) the target is: 1\n",
      "18: When input is tensor([52, 58,  1]) the target is: 58\n",
      "19: When input is tensor([52, 58,  1, 58]) the target is: 46\n",
      "20: When input is tensor([52, 58,  1, 58, 46]) the target is: 39\n",
      "21: When input is tensor([52, 58,  1, 58, 46, 39]) the target is: 58\n",
      "22: When input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is: 1\n",
      "23: When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is: 46\n",
      "24: When input is tensor([25]) the target is: 17\n",
      "25: When input is tensor([25, 17]) the target is: 27\n",
      "26: When input is tensor([25, 17, 27]) the target is: 10\n",
      "27: When input is tensor([25, 17, 27, 10]) the target is: 0\n",
      "28: When input is tensor([25, 17, 27, 10,  0]) the target is: 21\n",
      "29: When input is tensor([25, 17, 27, 10,  0, 21]) the target is: 1\n",
      "30: When input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is: 54\n",
      "31: When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is: 39\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension ( PyTorch convention: (B, T, C) = (Batch, Time, Channel))\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"{i}: When input is {context} the target is: {target}\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "\n",
    "We've built a simple bigram model in the earlier part of this series. But since the dataset is newer, and there are some slight tweaks in the implementation, I am reimplementing the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is: 4.8786\n",
      "Generated sequence: \n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLER\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # vocab_size X vocab_size lookup table\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        logits = self.token_embedding_table(idx) # logits.shape = (B, T, C) = (4, 8, 65) in our case\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T) # targets are of shape (B, T) \n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a tuple of sample indices of characters from where to start generating\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # logits.shape is (4, x, 65)\n",
    "\n",
    "            # we want the row corresponding to the last character in each batch to predict next character- i.e. the last elem in T dimension\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        \n",
    "        # idx will be the sequence generated for each batch\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Forward Pass:** Observe that for this bigram model, we don't have any context. So we can assume each character in each batch as a separate training example. For this model, the training examples are just one characters, as follows:\n",
    "\n",
    "When input is `tensor([24])` the target is: 43\n",
    "When input is `tensor([43])` the target is: 58\n",
    "\n",
    "What is happening with the forward pass is that for each of the characters in each of the batch, the forward pass basically plucks out a row from the `token_embedding_table`. Since our `vocab_size` is 65, for a batch we get `logits` of shape `(4, 8, 65)`. For each, for each character in the batch we are plucking out a row from the embedding table and interpreting this row as the `logits`.\n",
    "\n",
    "But there is one issue with this. PyTorch expects (B, C, ...) dimension in `F.cross_entropy()`. So we need to use `view` to change the shape f both the logits and the targets. Imagine it as a 3D cube. It helps a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note On Generate function:** What is the wishlist for the generate function? For each of the batch, we want to generate the next token. This next token is based only on the last character that we generated, and *not* the entire batch! We haven't yet added context yet.\n",
    "\n",
    "Further, we need to apply softmax to logits and draw a sample from it. And what we want is not just the next predicted token, but we want to add it to the current context which will be used to predict the next word again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro Tip:** For Adam, in practice, `lr=3e-4` works quite well. But for smaller datasets, you can have much faster learning rates like we are having. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is: 2.4321\n",
      "Generated sequence: \n",
      "\n",
      "Isow's higr tioy s ld ser.\n",
      "MIZd,\n",
      "Thadar bu, w fio anghab' me h g weithadwe'd ond sitolste venck.\n",
      "CUSCESeast flaive chalot f metstuplmeariloubeequrall \n"
     ]
    }
   ],
   "source": [
    "epochs = 10_000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=150)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly the outputs we are getting are not Shakespeare like, and we're never going to get them with a bigram model but this is a decent start from the untrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Self-Attention Block\n",
    "\n",
    "The bigram model was not paying any attention to the context. It was just considering the previous character. Now, we will build a self-attention block that pays attention to context of characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
