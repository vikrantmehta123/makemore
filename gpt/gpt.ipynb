{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GPT from Scratch\n",
    "\n",
    "In this notebook, I implement a GPT from scratch following Andrej Karpathy's YouTube series, along with my notes of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "epochs = 3000\n",
    "eval_interval = 300 \n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset\n",
    "\n",
    "We create the vocabulary and the dataset as we have been doing in the other `makemore` notebooks. But here, instead of using the `names` dataset, we will be using Tiny Shakespeare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of the dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are building a character level language model. Our vocabulary is going to be all the characters in the dataset, and the *tokens* in our language model are the characters mapped to integers. In LLMs, this tokenization could be at *subword* level, or something else also! \n",
    "\n",
    "The larger the vocabulary, the larger integer to token mapping you have. That means, that you can represent larger sentences using fewer tokens. On the contrary, if you have less number of tokens in your vocabulary, you will need more tokens to represent larger sentence. \n",
    "\n",
    "For example, with character level language model, we need `len(sentence)` tokens to represent it. But if we had a word level tokenization, then we would need `len(sentence.split(\" \"))` tokens, which would be fewer than the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size is:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab Size is: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 47, 6, 1, 58, 46, 43, 56, 43, 2]\n",
      "Hii, there!\n"
     ]
    }
   ],
   "source": [
    "# Create an integer to character mapping- i.e. the tokenizer that encodes and decodes tokens\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # takes an input string, and outputs a list of integers. i.e. the character map\n",
    "decode = lambda l: \"\".join([itos[i] for i in l]) # takes the token list, and produces the string for it\n",
    "\n",
    "print(encode(\"Hii, there!\"))\n",
    "print(decode(encode(\"Hii, there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the text into PyTorch tensor now, and split the encoded dataset into train and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(0.9 * len(data))\n",
    "train_data = data[:cut]\n",
    "validation_data = data[cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the context length first. This context length is the maximum context that the model can look at when making a prediction. However, there doesn't have to be 8 characters always- you can have less than that. Thus, you get something as this. But notice that now we're dealing with tokens and not integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length: maximum 8 tokens can be taken as context\n",
    "\n",
    "sample_x = train_data[:block_size]\n",
    "sample_y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = sample_x[:t+1]\n",
    "    target = sample_y[t]\n",
    "\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Batches\n",
    "\n",
    "Now that the dataset is there, we need to think about how the input text can be passed as a batch. \n",
    "\n",
    "Before that, an important thing to note about transformers is that there is a maximum number of tokens that you can pass to them. They are able to handle sequential inputs of arbitrary length, but this arbitrary length is also capped to some number such as 512. This number is the context length. You can have at maximum that many tokens but at minimum, you can have any number of tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about how would we create and pass a batch of sequences to the model. Our wishlist is the following:\n",
    "\n",
    "1. We want to pick arbitrary sequences so that the model can generalize well. How do we pick random sequences? Just pick out random starting indexes.\n",
    "2. How big a sequence should you pick? Well, it cannot be more than the context length of the model. For the moment, assume you would pick the input of size `block_size` i.e. the context size. For example, if you have a `batch_size` of 4 and `block_size` of 8, then you would randomly pick 4 indices in the dataset, and index 8 characters from that index. \n",
    "3. What should be the targets? The targets are just the next character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, one sequence of 8 characters gives us 8 training examples ( in cell above ). So when we have a batch of size 4, with each having a sequence of 32, it is going to give 32 training samples. \n",
    "\n",
    "**Important:** Each training sample can be passed independently to the transformer!\n",
    "\n",
    "The key is going to be figuring out how to pass this to the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs: torch.Size([4, 8])\n",
      "Shape of outputs: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split:str):\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # randomly select batch_size many indices. len(data) - block_size just handles edge case\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f\"Shape of inputs: {xb.shape}\")\n",
    "print(f\"Shape of outputs: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this batched input, we can again split the training examples. But note, this is NOT relevant till we get to transformers. At the moment, we are just training a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: When input is tensor([24]) the target is: 43\n",
      "1: When input is tensor([24, 43]) the target is: 58\n",
      "2: When input is tensor([24, 43, 58]) the target is: 5\n",
      "3: When input is tensor([24, 43, 58,  5]) the target is: 57\n",
      "4: When input is tensor([24, 43, 58,  5, 57]) the target is: 1\n",
      "5: When input is tensor([24, 43, 58,  5, 57,  1]) the target is: 46\n",
      "6: When input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is: 43\n",
      "7: When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is: 39\n",
      "8: When input is tensor([44]) the target is: 53\n",
      "9: When input is tensor([44, 53]) the target is: 56\n",
      "10: When input is tensor([44, 53, 56]) the target is: 1\n",
      "11: When input is tensor([44, 53, 56,  1]) the target is: 58\n",
      "12: When input is tensor([44, 53, 56,  1, 58]) the target is: 46\n",
      "13: When input is tensor([44, 53, 56,  1, 58, 46]) the target is: 39\n",
      "14: When input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is: 58\n",
      "15: When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is: 1\n",
      "16: When input is tensor([52]) the target is: 58\n",
      "17: When input is tensor([52, 58]) the target is: 1\n",
      "18: When input is tensor([52, 58,  1]) the target is: 58\n",
      "19: When input is tensor([52, 58,  1, 58]) the target is: 46\n",
      "20: When input is tensor([52, 58,  1, 58, 46]) the target is: 39\n",
      "21: When input is tensor([52, 58,  1, 58, 46, 39]) the target is: 58\n",
      "22: When input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is: 1\n",
      "23: When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is: 46\n",
      "24: When input is tensor([25]) the target is: 17\n",
      "25: When input is tensor([25, 17]) the target is: 27\n",
      "26: When input is tensor([25, 17, 27]) the target is: 10\n",
      "27: When input is tensor([25, 17, 27, 10]) the target is: 0\n",
      "28: When input is tensor([25, 17, 27, 10,  0]) the target is: 21\n",
      "29: When input is tensor([25, 17, 27, 10,  0, 21]) the target is: 1\n",
      "30: When input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is: 54\n",
      "31: When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is: 39\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension ( PyTorch convention: (B, T, C) = (Batch, Time, Channel))\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"{i}: When input is {context} the target is: {target}\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Self-Attention Block\n",
    "\n",
    "The bigram model was not paying any attention to the context. It was just considering the previous character. Now, we will build a self-attention block that pays attention to context of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention, Mathematically\n",
    "\n",
    "Think about what attention means. Attention is just a mechanism to let the past context interact with the current token. How do you make two vectors *interact*? By the way of addition and multiplication.\n",
    "\n",
    "One simple way of encoding context would be to take average of embeddings of all the previous tokens in the context, and remember that the context length is fixed. But what taking a simple average means is that the current token is paying equal attention to the all the previous tokens in the context. We don't want that. We want the attention to be data dependent, which in other words means that this attention should be *learned* from the model.\n",
    "\n",
    "So we're going to take a weighted average of the previous tokens but these weights are going to be learned by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights need to sum up to 1. And each token should only look at itself and the previous token- not the tokens that come after it.\n",
    "\n",
    "How do you encode this operation mathematically? Why, matrix multiplication, of course! If you think about it, if you have a lower diagonal weight matrix, then you can multiply it by the context and you're going to satisfy the condition that each token should only look at the token that comes before it.\n",
    "\n",
    "```\n",
    "weights @ context\n",
    "```\n",
    "\n",
    "In PyTorch, `torch.tril` gives us a lower triangular matrix. You need to pass the matrix that you want to convert to lower triangular.\n",
    "\n",
    "How do we encode a simple average with this matrix multiplication?\n",
    "Just divide each row by the number of non-zero entries in it. Think about it, in a lower triangular matrix, everything above the main diagonal is zero. So if you divide the each row by the number of non-zero entries in it, then essentially you are taking average. See the example below for the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do matrix multiplication to get the simple average. But think about the dimensions of this multiplication.\n",
    "\n",
    "```\n",
    "weights.shape = (T, T)\n",
    "x.shape = (B, T, C)\n",
    "```\n",
    "\n",
    "So, PyTorch is going to *slide* the matrix multiplication across the batch dimension and we're going to get an output that is of shape: ```(B, T, C)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = weights @ x\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main requirement is that these attention weights need to be trainable. For the moment, they are not. So we need a way that allows us to learn those attention weights while still preserving the averaging mechanism.\n",
    "\n",
    "That's where softmax is going to come in. Look up Mitesh Sir's Teacher Forcing or Masked Attention lectures for the intuition behind the `-inf` values. But essentially, these `-inf` and `0` values achieve the same thing as before:\n",
    "\n",
    "1. They let the model only focus on previous tokens\n",
    "2. Each row sums to 1.\n",
    "\n",
    "Because:\n",
    "1. $softmax(- \\infty) = 0$\n",
    "2. $exp(0) = 1$\n",
    "\n",
    "What we want is that the weights at each row should sum to 1 and they should be learnable. We know a function that lets generate a row that sums to 1 and that is softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # wherever tril == 0, replace with -inf\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can do matrix multiplication and we get the same matrix as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights @ x).allclose(xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a moment to appreciate what is happening here. For each token, we said we would combine the information from the previous tokens by addition and taking the average. With this mask, we effectively achieve the same. We prevent the information present in the future tokens to be combined with the present token because that information is going to get multiplied by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `weights` matrix is essentially what is going to be defining the how much attention to pay to each of the previous tokens given a certain token. We want the model to learn this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Self-Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's think about how self-attention is implemented. Each token (of the block_size tokens) *emits* two vectors- a key and a query. You'll introduce a Key and Query matrix as parameters which transform a given token. Then linear transformation of token with Key matrix gives the key vector and similarly for the query vector.\n",
    "\n",
    "- **Key Vector:** Captures \"What I contain\"\n",
    "- **Query Vector:** Captures \"What am I looking for\"\n",
    "\n",
    "So far, there is no communication between tokens.\n",
    "\n",
    "But then, the way we're going to find whether a token is worth paying attention to is by doing dot product of queries and keys. This is how we're going to find attention weights.   For each token, you want to find out whether the other tokens contain what the current token is looking for. If the dot product of these two vectors is high, then that means that the key vector contains what the query vector was looking for. \n",
    "\n",
    "This dot product is what the attention weights matrix will be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16 # hyperaparam\n",
    "key = nn.Linear(C, head_size, bias=False) # W_K matrix\n",
    "query = nn.Linear(C, head_size, bias=False) # W_Q matrix\n",
    "\n",
    "# Apply linear transforms to get key and query vectors for each\n",
    "k = key(x) # shape is (B, T, 16)\n",
    "q = query(x) # shape is (B, T, 16)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) # Transpose T, C of `k`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small comment on the matrix multiplication of `q` and `k`. This is essentially going to achieve pairwise dot product of queries and keys. But notice that we have a batch dimension. We don't want to use batch dimension when doing matrix multiplication. So we need to transpose only the last two dimensions. So the shape of weights now is: `(B, T, T)`. $ T \\times T$ is what we want- pairwise attention weights, and we have those pairwise attention weights for each the batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `weights`, we can do what we did before, we can mask it and apply softmax to get attention weighted aggregates of the previous tokens. Again, these are just weights to take a weighted average of the previous tokens such that the information in them is encoded in the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.0022e-01, 3.9978e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.4440e-01, 1.6887e-02, 7.3871e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.3022e-01, 1.8907e-01, 1.3135e-01, 4.9367e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.4424e-01, 3.1940e-01, 9.8877e-02, 2.7154e-01, 6.5939e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [8.9274e-02, 2.9868e-01, 2.6545e-02, 1.2551e-01, 5.7609e-02,\n",
       "          4.0238e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [1.5720e-01, 5.7396e-03, 3.6955e-01, 7.6793e-03, 3.3057e-01,\n",
       "          5.5747e-02, 7.3513e-02, 0.0000e+00],\n",
       "         [2.2850e-02, 5.7071e-01, 1.4504e-02, 9.3709e-03, 1.5510e-02,\n",
       "          2.2163e-01, 6.2442e-02, 8.2980e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.4241e-01, 5.7586e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.8207e-02, 4.0049e-01, 5.5130e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.5554e-01, 4.3620e-01, 1.4454e-01, 6.3721e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.1151e-01, 1.5597e-01, 1.2731e-01, 8.9787e-02, 2.1541e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.9618e-02, 2.6396e-02, 4.0200e-01, 2.1293e-01, 1.6313e-01,\n",
       "          1.4593e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0187e-02, 2.0304e-02, 5.6219e-02, 6.2446e-01, 5.4567e-02,\n",
       "          2.5137e-02, 1.9913e-01, 0.0000e+00],\n",
       "         [9.6692e-04, 1.9414e-03, 1.1671e-01, 8.5897e-01, 7.3919e-03,\n",
       "          6.7441e-03, 4.2830e-03, 2.9954e-03]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.8075e-01, 1.9253e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2305e-01, 2.3815e-01, 6.3880e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.2107e-02, 1.0774e-01, 4.9399e-01, 3.4615e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.7537e-02, 6.2576e-01, 1.3280e-01, 1.5798e-01, 1.5922e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.6390e-02, 1.5062e-01, 4.7755e-01, 8.5113e-02, 1.3767e-01,\n",
       "          5.2656e-02, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6629e-01, 5.3164e-02, 3.1286e-01, 1.4894e-01, 1.2456e-01,\n",
       "          7.7268e-02, 1.1692e-01, 0.0000e+00],\n",
       "         [2.2374e-02, 4.0271e-01, 1.7207e-02, 4.5397e-01, 7.7582e-03,\n",
       "          6.2270e-02, 1.6538e-02, 1.7170e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.5393e-01, 6.4607e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.7744e-01, 4.8640e-01, 3.6163e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.7780e-02, 2.9273e-02, 2.8506e-02, 9.2444e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.7126e-01, 7.7424e-02, 2.5513e-01, 3.1380e-02, 4.6481e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0920e-01, 6.2010e-01, 3.1534e-03, 1.3583e-02, 2.3080e-01,\n",
       "          2.3163e-02, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0379e-01, 1.0201e-01, 8.8418e-03, 3.6617e-02, 4.0813e-01,\n",
       "          1.2546e-01, 1.1515e-01, 0.0000e+00],\n",
       "         [1.6613e-01, 2.8119e-01, 1.8837e-02, 3.2277e-03, 2.0235e-01,\n",
       "          2.3744e-02, 2.0110e-01, 1.0341e-01]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "weights = weights.masked_fill(tril==0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ x\n",
    "\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `weights` now contains the attention weights to take the weighted average of only the past tokens and they are not uniform- so we have learnable parameters now!\n",
    "\n",
    "And `out` is now the modified input `x` by taking the weighted average using the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one additional component in the self attention block. That is, we don't modify the input `x` directly. We pass `x` through a linear transformation to produce a `value` for that input and then we modify this value. \n",
    "\n",
    "Another way of thinking about this is that the information contained in the `x` is private. So it tells the other vectors what it contains (key), what it is looking for (query), and what it will communicate with them (value). So we need a linear transform, which again are parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size)\n",
    "v = value(x)\n",
    "out = weights @ v\n",
    "out.shape # out shape is now 16D instead of 32D as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this Q, K, V matrix multiplications and the weighted aggregation constitutes one *head* of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andrej's Notes on Self-Attention\n",
    "\n",
    "1. Attention is just a communication mechanism- not specific to language modeling. You can look at it as a directed graph where each node points to other nodes with which it is communicating, and the weights of those edges? The attention weights! So you can apply attention to ANY directed graph!\n",
    "\n",
    "2. There is no notion of space in attention. Attention simply acts over a set of vectors. So if you have a problem where the sequence is important, then you need to encode the positional information, like we did.\n",
    "\n",
    "3. There is no communication across batches. With the matrix multiplications, the elements within a batch communicate but across batches there is no communication. \n",
    "\n",
    "4. Not all tasks require you to have a masked attention. In this task, we wanted to predict the future token and thus we didn't want the tokens to communicate with the future. But let's say your task is sentiment analysis, then there is no need for masking. You will have the entire sentiment with you and you can allow the communication with future tokens also. Without masking the self-attention block is called an \"encoder block\", and with masking it is called the \"decoder block\". Look up Mitesh sir's lectures on Encoder only and Decoder only models. Attention by itself has no notion about which nodes it can communicate with.\n",
    "\n",
    "5. Self-attention means that the keys, queries, and values are coming from same source- the input `x`. But there is cross-attention also where the queries come from different source and the keys & values come from an entirely different source, such as an encoder-decoder architecture.\n",
    "\n",
    "6. In the original \"Attention is All You Need\" paper, we have the equation:\n",
    "$softmax(\\dfrac{QK^T}{\\sqrt{d_k}})V$. We have done all the matrix multiplication part but there is also `sqrt(head_size)`. The attention is being *scaled* so it's called Scaled attention, and it is to make sure that when the input to Q and K is unit variance, then the `weights` is unit variance too, so as to avoid saturation when being passed through the softmax. So at least at initialization, you want `weights` to avoid taking too extreme values. So when we are doing the `q @ k` matrix multiplication, we need to multiply it by `C ** -0.5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Head Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32 # this is a global variable\n",
    "block_size = 8\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "\n",
    "        # TODO: need to understand indexing in tril for this line. It's most likely to handle less than block_size inputs\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf') )\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "\n",
    "        out = weights @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, whatever is not a parameter of the model is called a `buffer`. It is just creating the `tril` variable and assigning it to the module but it is not a parameter of the module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Language Model With Attention Head\n",
    "\n",
    "We introduce an additional linear layer called `lm_head` which will transform the output of attention head to match the dimensions of the targets. \n",
    "\n",
    "We also need to make sure that there are at maximum only `block_size` tokens that are passed to the model, otherwise the `position_embedding_table`, which captures the positional information upto block size, is going to throw an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro Tip:** Self-attention cannot handle very high learning rates. So you need something like `1e-3` or lower, and you need to train for more number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "block_size = 8\n",
    "\n",
    "class LangModelWithOneHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "\n",
    "        # Final linear layer to produce logits of equal dim as targets\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.sa_head(x) \n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # You can't have more than block_size tokens in context now because pos_embd will throw an error.\n",
    "            # so keep only the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, loss = self(idx_cond) # logits.shape is (4, x, 65)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        \n",
    "        # idx will be the sequence generated for each batch\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Attention Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 10_000\n",
    "eval_interval = 1000\n",
    "\n",
    "model = LangModelWithOneHead()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want a smoother loss estimate since the loss can vary batch to batch based on what sample is drawn. So we do what we did before to smooth the loss estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = { }\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # for train and val data, take mean of 300 iters\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    # set model back to train mode\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.2181, and val loss:4.2123\n",
      "Step 1000: train loss 2.8455, and val loss:2.8353\n",
      "Step 2000: train loss 2.5760, and val loss:2.6110\n",
      "Step 3000: train loss 2.5465, and val loss:2.4903\n",
      "Step 4000: train loss 2.4745, and val loss:2.5097\n",
      "Step 5000: train loss 2.4687, and val loss:2.5062\n",
      "Step 6000: train loss 2.4688, and val loss:2.4536\n",
      "Step 7000: train loss 2.4384, and val loss:2.4494\n",
      "Step 8000: train loss 2.4328, and val loss:2.4681\n",
      "Step 9000: train loss 2.4604, and val loss:2.4686\n",
      "Loss is: 2.6558\n",
      "\n",
      "Generated sequence: \n",
      "\n",
      "Sthithang angh drar did yo my pulas grires lounowm tor.\n",
      "\n",
      "Pr ing\n",
      "I acrevont, atheen thitanimis BEfe no.\n",
      " gep,\n",
      "Wif sere\n",
      "no age Your wlo, phourind ind wi\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {epoch}: train loss {losses['train']:.4f}, and val loss:{losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch from the dataset\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(f\"Loss is: {loss.item():.4f}\\n\")\n",
    "print(\"Generated sequence: \")\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=150)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is slightly better but it needs to be a whole lot better than this also, as the text being produced is still not as impressive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
